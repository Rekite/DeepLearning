{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow",
      "provenance": [],
      "collapsed_sections": [
        "yuRjT5FNEHpC",
        "kLs_KBwt3jEU",
        "4kEgDX5GOX8m",
        "syXnpoGP9cOm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2La73vYDx8k",
        "colab_type": "text"
      },
      "source": [
        "# For Learning ML with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCStNwKExVD",
        "colab_type": "code",
        "outputId": "f8faaa15-abcc-4058-a87c-ff846be4defa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm as tq\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eWTB5iQDNh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Linear Regression and Classification\n",
        "Cost function, Gradient descent, Mini batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuRjT5FNEHpC",
        "colab_type": "text"
      },
      "source": [
        "### 1-1. Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYoLrWGCY4e",
        "colab_type": "text"
      },
      "source": [
        "Cost function -> MSE <br>\n",
        "How to minimize **cost**?  -> Using Gradient Descent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3qN800EL5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_data = np.array([[1],[2],[3]], dtype=np.float32)\n",
        "# y_data = np.array([[1],[2],[3]], dtype=np.float32)\n",
        "x_data = [1,2,3]\n",
        "y_data = [1,2,3]\n",
        "\n",
        "X = tf.placeholder(tf.float32, name='X_input')\n",
        "Y = tf.placeholder(tf.float32, name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "#Model\n",
        "hypothesis = X*W + b \n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis-Y)) #Cost function\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "## If you want to get gradients\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "# gvs = optimizer.compute_gradients(cost)\n",
        "# apply_gradients = optimizer.apply_gradients(gvs)\n",
        "# sess.run(apply_gradients)\n",
        "\n",
        "with tf.Session() as sess: ## Tensorflow 1에서는 Session을 이용해 계산해 주어야 한다.\n",
        "  sess.run(tf.global_variables_initializer()) ##Initializes global variables in the graph.\n",
        "\n",
        "  for epoch in tq(range(2001)):\n",
        "    sess.run(optimizer, feed_dict={X:x_data,Y:y_data})\n",
        "\n",
        "    if epoch%100 == 0:\n",
        "      print(f'{epoch} W : {sess.run(W)}, b : {sess.run(b)} loss : {sess.run(cost,feed_dict={X:x_data,Y:y_data})}')\n",
        "\n",
        "line_x = np.arange(min(x_data),max(x_data),0.01)\n",
        "line_y = W*line_x + b\n",
        "plt.plot(line_x,line_y,'-r')\n",
        "plt.plot(X,Y,'bo')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRINi10gnP8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow 2\n",
        "# X = [1,2,3]\n",
        "# Y = [1,2,3]\n",
        "\n",
        "# # Model\n",
        "# W = tf.Variable(tf.random.normal([1]), name='weight')\n",
        "# b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "# def compute_cost():\n",
        "#   y_pred = W * X + b\n",
        "#   cost = tf.reduce_mean(tf.square(Y-y_pred))\n",
        "#   return cost\n",
        "\n",
        "# optimizer = tf.optimizers.SGD(learning_rate = 0.01)\n",
        "\n",
        "# for i in tq(range(1000)):\n",
        "#   optimizer.minimize(compute_cost, [W,b])\n",
        "\n",
        "#   if i%100 == 0:\n",
        "#     print(f'{i} W : {W.numpy()}, b : {b.numpy()} loss : {compute_cost().numpy()}')\n",
        "\n",
        "# line_x = np.arange(min(X),max(X),0.01)\n",
        "# line_y = W*line_x + b\n",
        "\n",
        "# plt.plot(line_x,line_y,'-r')\n",
        "# plt.plot(X,Y,'bo')\n",
        "# plt.xlabel('X')\n",
        "# plt.ylabel('Y')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rv74lIf3f56",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLs_KBwt3jEU",
        "colab_type": "text"
      },
      "source": [
        "###1-2. Multi Varialbe Linear Regression\n",
        "with mini-batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oswv8F4hFYQE",
        "colab_type": "text"
      },
      "source": [
        "Lecture(theory) : H(x) = Wx + b <br>\n",
        "In TensorFlow : H(X) = XW -> Because of matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcY4tWk_4LKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[73., 80., 75.],\n",
        "          [93., 88., 93.],\n",
        "          [89., 91., 90.],\n",
        "          [96., 98., 100.],\n",
        "          [73., 66., 70.]]\n",
        "y_data = [[152.],\n",
        "          [185.],\n",
        "          [180.],\n",
        "          [196.],\n",
        "          [142.]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,3], name='X_input')\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1], name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b # tf.matmaul => 행렬곱\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        " sess.run(tf.global_variables_initializer())\n",
        "\n",
        " for epoch in tq(range(2001)):\n",
        "   cost_val, hy_val, _ = sess.run([cost, hypothesis, train_step], feed_dict={X:x_data,Y:y_data})\n",
        "   \n",
        "   if epoch % 100 == 0:\n",
        "     print(f'{epoch}  | Cost: {cost_val}\\nPrediction:\\n{hy_val}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdZtV_c7Lk54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data load using tf reader\n",
        "\n",
        "filenames = tf.train.string_input_producer(['xxxxx.csv'], \n",
        "                                                shuffle=False, name='filenames')\n",
        "\n",
        "key, value = tf.TextLineReader().read(filenames)\n",
        "\n",
        "record_defaults = [[0.], [0.], [0.], [0.]]\n",
        "data = tf.decode_csv(value, record_defaults=record_defaults)\n",
        "\n",
        "train_x_batch, train_y_batch = tf.train.batch([data[:-1], data[-1:]], batch_size=10)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,3], name='X_input')\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1], name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Start population the filenames, Batch를 관리하기 위한 것 같음. 통상적으로 이렇게 사용\n",
        "  coord = tf.train.Coordinator()\n",
        "  threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
        "\n",
        "  for epoch in range(2001):\n",
        "      x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
        "      cost_val, hy_val, _ = sess.run(\n",
        "          [cost, hypothesis, train_step], feed_dict={X: x_batch, Y: y_batch})\n",
        "      if epoch % 100 == 0:\n",
        "        print(f'{epoch}  | Cost: {cost_val}\\nPrediction:\\n{hy_val}')\n",
        "\n",
        "  coord.request_stop()\n",
        "  coord.join(threads)\n",
        "\n",
        "\n",
        "## Estimate the value through the model created.\n",
        "print(\"Other scores will be \",\n",
        "      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kEgDX5GOX8m",
        "colab_type": "text"
      },
      "source": [
        "###1-3. Logistic Classification\n",
        "predict 0 or 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgouQmT_Oxl1",
        "colab_type": "text"
      },
      "source": [
        "Using simoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjnQBDjMObSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
        "y_data = [[0],[0],[0],[1],[1],[1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# 값을 0~1 사이로 나오게 하기 위해 sigmoid 함수를 취해줌\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "# cost/loss function\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "#cast Y_pred to 0 or 1\n",
        "y_pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, Y), dtype=tf.float32))\n",
        "\n",
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for eopch in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train_step], feed_dict={X: x_data, Y: y_data})\n",
        "        if epoch % 200 == 0:\n",
        "            print(epoch, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, c, a = sess.run([hypothesis, y_pred, accuracy],\n",
        "                       feed_dict={X: x_data, Y: y_data})\n",
        "    print(f\"\\nHypothesis: '{h}'\\nCorrect (Y): '{c}'\\nAccuracy: '{a}' \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syXnpoGP9cOm",
        "colab_type": "text"
      },
      "source": [
        "##2. Softmax Regression\n",
        "predict 0 to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsa4mrH8974E",
        "colab_type": "text"
      },
      "source": [
        "softmax has predict value as probability.\n",
        "The sum of all probabilities is 1. <br>\n",
        "Cost function : Cross - Entropy.\n",
        "Optimizer : GD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foyH1_MfAiMI",
        "colab_type": "text"
      },
      "source": [
        "Softmax는 이진화된 값(Binary Value)으로 표현하기 위해 One-Hot Encoding이라는 것을 해주어야 한다. <br>\n",
        "단순한 Integer Encoding의 범주형 값(Categorical Value)은 잘못된 경향성을 학습할 수 있다. EX) dog(1), cat(2), horse(3)이 있을 때 dog와 horse의 평균을 cat이라 생각할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlaQ3Rk6AKS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "nb_classes = 7\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 16])\n",
        "Y = tf.placeholder(tf.int32, [None, 1])\n",
        "\n",
        "# In TF, if the input indices is rank N, the output will have rank N+1.\n",
        "# So you have to reshape the output.\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "logits = tf.matmul(X, W) + b\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "# tf.nn.softmax_cross_entropy_with_logits API -> One-hot Encoding 형태의 정답 레이블일 때 사용.\n",
        "# tf.nn.sparse_softmax_corss_entropy_with_logits API -> One-hot Encoding을 자동으로 수행함으로 labels에 범주형 값 대입.\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf.stop_gradient([Y_one_hot])))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# tf.argmax는 probability중 가장 높은 probabliity를 스칼라 형태로 리턴.\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(Y_one_hot,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(2001):\n",
        "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "                                        \n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
        "\n",
        "    # Let's see if we can predict\n",
        "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
        "    for p, y in zip(pred, y_data.flatten()):\n",
        "        print(f\"[{p==int(y)}] Prediction: {p} True Y: {int(y)}\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_GjLw_MKtCt",
        "colab_type": "text"
      },
      "source": [
        "##3. Learning rate, Preprocessing(Normalization), Overfitting(Regularization), Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3IDmy8OL3mj",
        "colab_type": "text"
      },
      "source": [
        "#### Learning rate\n",
        "* Large learning rate - Cost가 계속 증가한다.\n",
        "* Small learning rate - Cost 변화가 너무 없다.\n",
        "` 결국 Learning rate은 Cost를 관찰하여 직접 설정해햐 한다.`\n",
        "-----\n",
        "#### Preprocessing(Normalization)\n",
        "* zero-centered data : value의 중심을 0으로 바꿔주는 것\n",
        "* normalized data : value값의 범위를 정해주는 것, 너무 튀는 값은 train data에서 제거해준다.\n",
        "  > Standardization(normalization의 한 종류)\n",
        "  > ```python\n",
        "  > x_std[:,0] = (x[:,0] - x[:,0].mean()) / x[:,0].std()\n",
        "  > ```\n",
        "  > std() = 표준편차\n",
        "------\n",
        "#### Overfitting\n",
        "Regularization은 Overfitting을 줄이기 위해 무거운 weight을 줄여주는 방식. cost함수 뒤에 텀을 추가. Regularization strength를 설정.\n",
        "```python\n",
        "l2reg = Regularization_strength * tf.reduce_sum(tf.square(W))\n",
        "```\n",
        "-----\n",
        "#### Training Data, Validation Data, Test data\n",
        "전체 데이터를 `트레이닝`, `검증`, `테스트`의 총 3가지로 데이터를 분류하는 것. 검증용 데이터는 트레이닝 과정에서 학습에 사용하지는 않지만 중간중간 테스트하는데 사용해서 학습하고있는 모델이 오버피팅에 빠지지 않는지 체크한다. \n",
        "그리고 마지막으로 테스트 데이터를 통해 모델의 정확도를 확인한다.\n"
      ]
    }
  ]
}